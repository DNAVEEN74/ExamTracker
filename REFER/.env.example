# ─── Cloudflare R2 (S3-compatible storage) ───────────────────────────────────
# Get these from your Cloudflare Dashboard → R2 → Manage R2 API tokens
R2_ENDPOINT=https://<YOUR_ACCOUNT_ID>.r2.cloudflarestorage.com
R2_ACCESS_KEY_ID=your_r2_access_key_id
R2_SECRET_ACCESS_KEY=your_r2_secret_access_key
R2_BUCKET=examtracker-notifications

# ─── Scraper Behavior ─────────────────────────────────────────────────────────
# Which priority levels to scrape in this cron run
# P0 = every 2hrs | P1 = every 6hrs | P2 = every 12hrs | P3 = every 24hrs
PRIORITY_FILTER=P0,P1

# Concurrency — how many sites to hit in parallel
# Keep at 3 to avoid being blocked by government servers
SCRAPE_CONCURRENCY=3

# Delay between requests in milliseconds (be respectful)
REQUEST_DELAY_MS=2000

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=30000

# ─── Storage ─────────────────────────────────────────────────────────────────
# Where to store page hashes (for change detection)
# In production, replace with Redis (Upstash) for distributed scraping
HASH_STORE_PATH=./data/hashes.json

# ─── Parsing Pipeline ─────────────────────────────────────────────────────────
# Webhook URL to call when a new PDF is found and uploaded to R2
# This triggers your Claude AI parsing worker
# Leave empty in dev to write to local parse_queue.jsonl instead
PARSING_WEBHOOK_URL=https://your-api.examtracker.in/api/internal/parse-queue

# ─── Logging ─────────────────────────────────────────────────────────────────
LOG_LEVEL=info
